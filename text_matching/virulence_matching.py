"""
Compare some lists of functions against the virulence factor database and see if we can come up with a score
for the lists.

We start with the clusters of data generated by the CF Autoencoders, and the virulence factor database as a text file.
"""

import os
import sys
import gzip
import json
import argparse
import re
from typing import Any

import numpy as np
#from fuzzywuzzy import fuzz

# this list includes some common biological words and the list of english stopwords from NLTK

common_vfdb_words = {'acid', 'acids', 'amino', 'bacillus', 'bacteria', 'bacterial', 'betaine',
                     'biogenesis', 'biosynthesis', 'catabolism', 'cell', 'class', 'cluster', 'complex', 'containing',
                     'cycle', 'degradation', 'division', 'efflux', 'elongation', 'enzymes', 'esat6', 'factors', 'fatty',
                     'including', 'lipid', 'locus', 'membrane', 'modification',
                     'operon', 'outer', 'pathway', 'peptide', 'phosphate', 'processing', 'proteins',
                     'reductase', 'regulation', 'regulator', 'regulatory', 'repair', 'resistance', 'response',
                     'secretion', 'signal', 'sporulation', 'stress', 'subunits', 'sulfur', 'synthase',
                     'synthesis', 'systems', 'targets', 'transcription', 'transporter', 'twocomponent', 'uptake',
                     'utilization', 'wall'}

bio_stopwords = {'putative', 'predicted', 'family', 'domain', 'protein', 'hypothetical',
    'transport', 'binding', 'component', 'subunit', 'associated',
    'expression', 'activity', 'involved', 'like', 'type', 'system',
    'encoded', 'region', 'related', 'factor', 'superfamily', 'gene'}

nltk_topwords= {'a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', "aren't", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', "couldn't", 'd', 'did', 'didn', "didn't", 'do', 'does', 'doesn', "doesn't", 'doing', 'don', "don't", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', "hadn't", 'has', 'hasn', "hasn't", 'have', 'haven', "haven't", 'having', 'he', "he'd", "he'll", "he's", 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', "i'd", "i'll", "i'm", "i've", 'if', 'in', 'into', 'is', 'isn', "isn't", 'it', "it'd", "it'll", "it's", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', "mightn't", 'more', 'most', 'mustn', "mustn't", 'my', 'myself', 'needn', "needn't", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', "shan't", 'she', "she'd", "she'll", "she's", 'should', "should've", 'shouldn', "shouldn't", 'so', 'some', 'such', 't', 'than', 'that', "that'll", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', "they'd", "they'll", "they're", "they've", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', "wasn't", 'we', "we'd", "we'll", "we're", "we've", 'were', 'weren', "weren't", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', "won't", 'wouldn', "wouldn't", 'y', 'you', "you'd", "you'll", "you're", "you've", 'your', 'yours', 'yourself', 'yourselves'}

stopwords = common_vfdb_words.union(bio_stopwords).union(nltk_topwords)

__author__ = 'Rob Edwards'

bacterial_pathogens = {
    'kingdom': ['Bacteria'],
    'phylum': ['Chlamydiota', 'Actinomycetota', 'Pseudomonadota', 'Mycoplasmatota', 'Spirochaetota', 'Bacillota',
               'Campylobacterota'],
    'class': ['Chlamydiia', 'Spirochaetia', 'Epsilonproteobacteria', 'Bacilli', '', 'Betaproteobacteria',
              'Gammaproteobacteria', 'Actinomycetes', 'Alphaproteobacteria', 'Clostridia'],
    'order': ['Vibrionales', 'Moraxellales', 'Bacillales', 'Rickettsiales', 'Campylobacterales', 'Mycoplasmoidales',
              'Enterobacterales', 'Actinomycetales', 'Eubacteriales', 'Neisseriales', 'Lactobacillales',
              'Spirochaetales', 'Pasteurellales', 'Hyphomicrobiales', 'Mycobacteriales', 'Pseudomonadales',
              'Burkholderiales', 'Thiotrichales', 'Chlamydiales', 'Legionellales'],
    'family': ['Neisseriaceae', 'Corynebacteriaceae', 'Campylobacteraceae', 'Chlamydiaceae', 'Actinomycetaceae',
               'Streptococcaceae', 'Rickettsiaceae', 'Nocardiaceae', 'Vibrionaceae', 'Moraxellaceae', 'Francisellaceae',
               'Yersiniaceae', 'Comamonadaceae', 'Mycoplasmoidaceae', 'Borreliaceae', 'Legionellaceae',
               'Pseudomonadaceae', 'Clostridiaceae', 'Burkholderiaceae', 'Pasteurellaceae', 'Helicobacteraceae',
               'Listeriaceae', 'Alcaligenaceae', 'Bacillaceae', 'Staphylococcaceae', 'Brucellaceae', 'Mycobacteriaceae',
               'Enterococcaceae', 'Enterobacteriaceae'],
    'genus': ['Burkholderia', 'Streptococcus', 'Acinetobacter', 'Pasteurella', 'Klebsiella', 'Staphylococcus',
              'Listeria', 'Vibrio', 'Francisella', 'Bacillus', 'Moraxella', 'Shigella', 'Helicobacter', 'Pseudomonas',
              'Rickettsia', 'Neisseria', 'Mycobacterium', 'Clostridium', 'Borreliella', 'Mycoplasmoides', 'Yersinia',
              'Haemophilus', 'Enterococcus', 'Actinomyces', 'Legionella', 'Chlamydia', 'Corynebacterium', 'Escherichia',
              'Brucella', 'Hydrogenophaga', 'Campylobacter', 'Nocardia', 'Bordetella'],
    'species': ['Bacillus anthracis', 'Mycobacterium tuberculosis', 'Nocardia abscessus', 'Burkholderia cenocepacia',
                'Brucella melitensis', 'Enterococcus faecalis', 'Streptococcus pyogenes', 'Borreliella burgdorferi',
                'Legionella pneumophila', 'Pasteurella multocida', 'Escherichia coli', 'Neisseria gonorrhoeae',
                'Listeria monocytogenes', 'Shigella dysenteriae', 'Rickettsia prowazekii', 'Staphylococcus aureus',
                'Chlamydia trachomatis', 'Helicobacter pylori', 'Streptococcus pneumoniae', 'Neisseria meningitidis',
                'Bordetella pertussis', 'Acinetobacter baumannii', 'Mycoplasmoides pneumoniae', 'Vibrio cholerae',
                'Pseudomonas aeruginosa', 'Actinomyces israelii', 'Francisella tularensis',
                'Corynebacterium diphtheriae', 'Hydrogenophaga pseudoflava', 'Clostridium formicaceticum',
                'Clostridium botulinum', 'Yersinia pestis', 'Campylobacter jejuni', 'Haemophilus influenzae',
                'Klebsiella pneumoniae', 'Clostridium perfringens', 'Moraxella catarrhalis']
}


def normalize_and_tokenize(term, split_words=False):
    term = term.lower()
    term = re.sub(r'[^a-z0-9\s]', '', term)  # remove punctuation
    if split_words:
        term = term.split()
        term = [x for x in term if len(x) > 3 and x not in stopwords]  # remove short words
        return set(term)
    return {term}


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description=' ')
    parser.add_argument('-j', help='json file of clusters from autoencoder', default='clusters.json.gz')
    parser.add_argument('-f', help='virulence factor database file', default='vfdb.txt.gz')
    parser.add_argument('-t', help='bacterial taxa used in the clusters', default='family')
    parser.add_argument('-o', help='output file', default='cluster_virulence_score.tsv')
    parser.add_argument('-s', help='split words in tokens', action='store_true')
    parser.add_argument('-v', help='verbose output', action='store_true')
    args = parser.parse_args()

    total_vocab = set()
    virfuncs = set()
    with gzip.open(args.f, 'rt', encoding='latin1') as f:
        for l in f:
            p = l.strip().split("\t")
            if len(p) < 3:
                continue
            if p[1]:
                virfuncs.update(normalize_and_tokenize(p[1], args.s))
            if p[2]:
                virfuncs.update(normalize_and_tokenize(p[2], args.s))
            if len(p) > 6 and p[6]:
                virfuncs.update(normalize_and_tokenize(p[6], args.s))

    if args.t in bacterial_pathogens:
        taxaset = set()
        for t in bacterial_pathogens[args.t]:
            taxaset.update(normalize_and_tokenize(t, args.s))
        virfuncs.update(taxaset)
    total_vocab.update(virfuncs)

    if False and args.v:
        vflist = "\n".join(list(virfuncs))
        print(f"Found {len(virfuncs)} virulence factors in the database\n{vflist}", file=sys.stderr)

    if args.v:
        print(f"Found {len(virfuncs)} virulence factors in the database", file=sys.stderr)

    with gzip.open(args.j, 'rt', encoding='latin1') as f:  # 'rt' means read text
        clusters = json.load(f)

    cluster_tokens = {}
    for c in clusters:
        tokens = set()
        for f in clusters[c]:
            tokens.update(normalize_and_tokenize(f, args.s))
        cluster_tokens[c] = tokens
        total_vocab.update(tokens)

    if args.v:
        print(f"Found {len(total_vocab)} total tokens", file=sys.stderr)

    words_in_common = {}
    word_cluster_count = {}
    with open(args.o, 'w') as out:
        print(
            "Cluster\tTokens\tVirulence Factors\tIntersection\tUnion\tJaccard Distance\tFrac cluster virulence\tEnrichment Score\tNormalized Enrichment Score",
            file=out)
        for c in clusters:
            tokens = cluster_tokens[c]
            words_in_common[c] = list(tokens.intersection(virfuncs))
            for w in words_in_common[c]:
                if w not in word_cluster_count:
                    word_cluster_count[w] = set()
                word_cluster_count[w].add(c)
            observed_matches = len(tokens.intersection(virfuncs))
            expected_matches = len(tokens) * (len(virfuncs) / len(total_vocab))
            enrichment_score = (
                                           observed_matches - expected_matches) / expected_matches if expected_matches > 0 else np.nan
            enrichment2 = (observed_matches - expected_matches) / (observed_matches + expected_matches) if (
                                                                                                                       observed_matches + expected_matches) > 0 else np.nan

            print("\t".join(map(str, [c,
                                      len(tokens),
                                      len(virfuncs),
                                      len(tokens.intersection(virfuncs)),
                                      len(tokens.union(virfuncs)),
                                      len(tokens.intersection(virfuncs)) / len(tokens.union(virfuncs)),
                                      len(tokens.intersection(virfuncs)) / len(tokens),
                                      enrichment_score,
                                      enrichment2])),
                  file=out)

    with open("virulence_factors_in_clusters.json", 'w') as out:
        json.dump(words_in_common, out, indent=4)

    for w in word_cluster_count:
        if len(word_cluster_count[w]) > 3:
            #print(f"{w}: {len(word_cluster_count[w])} clusters")
            print(f"'{w}', ", end="")
    print()